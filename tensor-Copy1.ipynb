{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "import collections\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 2 3 3 2 1 3 4 3 3 3 1 4 4 0 3 3 0 2 3 3 3 1 4 1 1 3 3 3 2 3 1 3 3 1\n",
      " 0 0 1 1 1 1 3 1 3 3 0 0 1 3 4 3 1 3 1 1 1 3 1 3 3 3 1 1 1 3 3 4 4 3 3 1 1\n",
      " 1 3 3 1 3 4 2 3 3 3 3 1 3 1 3 3 1 2 3 1 3 3 1 1 3 3 3 3 3 1 3 3 1 3 3 3 3\n",
      " 3 3 4 1 1 3 3 1 3 3 1 1 1 3 1 3 1 1 3 1 4 3 3 1 3 1 3 4 1 0 3 1 1 3 1 1 1\n",
      " 3 1 3 3 4 4 2 3 1 1 2 4 3 1 1 4 1 4 4 1 3 3 3 1 3 3 1 1 4 1 3 3 3 1 3 1 2\n",
      " 3 1 2 3 3 1 3 1 3 3 4 3 1 1 4 1 1 3 1 3 1 4 4 3 3 2 1 1 3 4 1 4 4 3 1 1 3\n",
      " 2 1 4 4 4 3 3 4 1 2 3 1 1 1 1 4 1 3 3 4 1 1 3 3 3 1 4 3 1 3 1 3 1 3 3 3 1\n",
      " 4 3 2 3 3 1 1 3 1 3 1 1 2 3 3 1 1 3 3 3 3 1 3 3 1 1 3 3 1 1 3 3 3 4 1 1 3\n",
      " 3 3 3 1 3 1 1 1 1 1 1 1 1 1 3 1 1 3 3 3 3 1 3 3 3 1 3 3 3 1 3 2 3 4 3 4 1\n",
      " 4 1 1 1 3 3 1 4 1 2 2 4 3 3 1 3 1 3 1 3 3 3 3 3 3 4 3 3 3 4 1 3 1 1 4 1 1\n",
      " 1 1 1 1 1 1 3 1 1 3 1 4 1 3 2 3 1 3 1 2 3 3 3 1 1 1 3 3 1 1 1 3 3 3 3 3 3\n",
      " 1 1 3 3 3 3 4 3 4 4 2 2 3 1 3 3 1 3 1 3 3 1 3 3 1 3 3 3 3 1 4 1 3 1 3 2 1\n",
      " 3 1 1 3 1 3 1 1 1 3 1 3 3 1 1 1 1 3 3 4 1 1 3 1 4 1 1 3 4 3 3 3 4 1 3 3 2\n",
      " 1 3 4 4 1 1 1 1 3 3 1 4 3 1 1 3 3 3 1 1 2 1 3 2 4 1 3 1 1 2 3 4 2 3 1 1 3\n",
      " 3 4 4 1 3 3 1 1 3 1 1 3 3 4 1 3 4 1 4 4 3 3 3 1 3 4 1 3 1 3 1 3 1 2 3 1 1\n",
      " 1 3 1 1 3 1 1 3 3 1 3 1 3 3 3 1 1 1 3 1 4 1 3 1 3 3 3 3 3 1 4 4 3 3 3 3 1\n",
      " 1 1 3 3 3 1 1 3 1 1 1 1 3 4 1 1 3 3 2 1 1 3 4 3 3 1 1 1 3 4 1 4 1 1 3 3 3\n",
      " 3 1 3 1 1 3 3 1 3 3 3 3 2 3 1 1 3 3 1 1 3 2 3 3 4 3 1 1 1 4 1 1 4 3 3 3 3\n",
      " 3 4 3 1 4 2 2 1 4 4 3 1 3 3 1 1 3 3 4 3 4 3 3 1 3 4 3 3 1 1 1 3 3 4 1 1 3\n",
      " 1 3 1 2 1 3 2 3 3 3 1 1 2 1 3 3 4 3 3 2 1 1 1 1 1 3 4 1 3 3 1 3 1 3 1 3 1\n",
      " 4 1 3 1 1 1 1 2 3 1 2 3 3 3 3 3 3 1 1 3 3 3 2 3 3 3 3 3 1 3 3 1 1 1 3 2 3\n",
      " 3 4 1 3 3 3 3 1 1 3 3 1 1 3 1 3 3 3 1 2 2 1 1 3 1 3 3 3 3 3 3 1 3 3 1 4 3\n",
      " 3 1 2 3 4 1 3 4 2 3 2 4 1 3 3 1 1 1 2 3 1 3 4 2 2 4 1 3 3 3 3 3 4 1 3 3 1\n",
      " 1 3 4 1 3 1 3 3 3 1 3 1 3 2 2 3 1 1 1 1 3 3 3 4 4 1 3 1 1 1 1 4 3 3 3 1 1\n",
      " 2 3 3 1 3 3 3 1 3 3 3 3 1 1 3 4 3 3 3 3 3 2 1 1 1 3 3 1 3 3 3 3 3 3 1 3 3\n",
      " 1 1 1 2 3 2 2 1 1 3 1 1 3 1 1 1 4 3 1 1 3 1 3 4 3 3 3 3 2 3 3 3 3 3 0 3 1\n",
      " 3 3 3 4 1 3 3 3 1 3 1 3 3 1 3 3 1 3 3 1 1 1 3 3 1 3 3 3 3 1 4 1 3 3 3 2 3]\n"
     ]
    }
   ],
   "source": [
    "# define columns\n",
    "headers = ['business_classification', 'business_impact', 'business_relevance',\n",
    "       'category_id', 'data_confidentiality', 'eoldriver', 'extensibility',\n",
    "       'geographical_scope', 'has_dependencies', 'id', 'io_intensity',\n",
    "       'latency_sensitivity', 'no_of_users', 'scalability', 'service_level',\n",
    "       'source_code_available', 'stage', 'type', 'user_facing',\n",
    "       'workload_variation', 'dependencies.Hardware.Dependent',\n",
    "       'dependencies.Operating.Environment.Dependent',\n",
    "       'dependencies.Operating.System.Dependent', 'IsPassPlatAvail',\n",
    "       'IsHarwareSupported', 'IsOSSupported', 'IsPlatformSupported',\n",
    "       'IsDatabaseSupported']\n",
    "\n",
    "#read csv\n",
    "ds = pd.read_csv('completeData.csv', usecols = headers)\n",
    "\n",
    "categories={}\n",
    "for f in headers:\n",
    "    ds[f] = ds[f].astype('category')\n",
    "    categories[f] = ds[f].cat.categories\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = pd.read_csv('completeData.csv', usecols = ['pivot.disposition_1'])\n",
    "#print(labels)\n",
    "#covert strings into numericals\n",
    "df = pd.get_dummies(ds, columns = headers)\n",
    "#print(df)\n",
    "df_num, df_labels = pd.factorize(labels['pivot.disposition_1'])\n",
    "print(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df_num)\n",
    "stack = pd.DataFrame(np.column_stack((df_num, df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1100379"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split data\n",
    "df_train, df_test = train_test_split(stack, test_size=0.25)\n",
    "\n",
    "num_train_entries = df_train.shape[0]\n",
    "num_train_features = df_train.shape[1]-1\n",
    "\n",
    "num_test_entries = df_test.shape[0]\n",
    "num_test_features = df_test.shape[1]-1\n",
    "\n",
    "#create temp csv files\n",
    "df_train.to_csv('train_temp.csv', index=False)\n",
    "df_test.to_csv('test_temp.csv', index=False)\n",
    "\n",
    "open('acat_train.csv', 'w').write(str(num_train_entries)+','+str(num_train_features)+','+open('train_temp.csv').read())\n",
    "open('acat_test.csv', 'w').write(str(num_test_entries)+','+str(num_test_features)+','+open('test_temp.csv').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Dataset = collections.namedtuple('Dataset', ['data', 'target'])\n",
    "Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "def load_csv(filename, target_dtype, target_column=-1, has_header=True):\n",
    "    \"\"\"Load dataset from CSV file.\"\"\"\n",
    "    with tf.gfile.Open(filename) as csv_file:\n",
    "        data_file = csv.reader(csv_file)\n",
    "        if has_header:\n",
    "            header = next(data_file)\n",
    "            n_samples = int(header[0])\n",
    "            n_features = int(header[1])\n",
    "            data = np.empty((n_samples, n_features))\n",
    "            target = np.empty((n_samples,), dtype=np.int)\n",
    "            for i, ir in enumerate(data_file):\n",
    "                target[i] = np.asarray(int(eval(ir.pop(target_column))), dtype=target_dtype)\n",
    "                data[i] = np.asarray(ir, dtype=np.float64)\n",
    "        else:\n",
    "            data, target = [], []\n",
    "            for ir in data_file:\n",
    "                target.append(ir.pop(target_column))\n",
    "                data.append(ir)\n",
    "    return Dataset(data=data, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data into tf\n",
    "import csv\n",
    "training_set = load_csv(filename = 'acat_train.csv', target_dtype = np.int, target_column=0)\n",
    "test_set = load_csv(filename = 'acat_test.csv', target_dtype = np.int, target_column=0)\n",
    "# import collections\n",
    "# Dataset = collections.namedtuple('Dataset', ['data', 'target'])\n",
    "# Datasets = collections.namedtuple('Datasets', ['train', 'validation', 'test'])\n",
    "\n",
    "# def create_dataset(da, samples, features, target_column = -1):\n",
    "# #     data = np.empty((samples, features))\n",
    "# #     target = np.empty((samples,), dtype = np.int)\n",
    "# #     for i, ir in enumerate(da):\n",
    "# #         target[i] = np.asarray(ir.pop(target_column), dtype = np.int)\n",
    "# #         data[i] = np.asarray(ir, dtype = np.int)\n",
    "#         data, target = [], []\n",
    "#         for ir in da:\n",
    "#             target.append(ir.pop(target_column), dtype = np.int)\n",
    "#             data.append(ir)\n",
    "#         return Dataset(data=data, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model directory = C:\\Users\\A664107\\AppData\\Local\\Temp\\tmpo335zwhp\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x00000000112494A8>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_model_dir': 'C:\\\\Users\\\\A664107\\\\AppData\\\\Local\\\\Temp\\\\tmpo335zwhp', '_save_checkpoints_steps': None, '_task_id': 0, '_environment': 'local', '_tf_random_seed': None, '_num_worker_replicas': 0, '_session_config': None, '_master': '', '_keep_checkpoint_max': 5, '_evaluation_master': '', '_task_type': None, '_save_summary_steps': 100, '_num_ps_replicas': 0, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=28)]\n",
    "\n",
    "model_dir = tempfile.mkdtemp() \n",
    "print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns, hidden_units=[10, 20, 10], \\\n",
    "                                            n_classes=5, model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define tf variables\n",
    "\n",
    "#training_set = create_dataset(da=df_train, samples=num_train_entries, target_column=0, features=num_train_features)\n",
    "#test_set = create_dataset(da=df_test, samples=num_test_entries, target_column=0, features=num_test_features)\n",
    "def get_train_const():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "    return x, y\n",
    "def get_test_const():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-e843a3cd33e0>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-9-e843a3cd33e0>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\A664107\\AppData\\Local\\Temp\\tmpo335zwhp\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.57468\n",
      "INFO:tensorflow:global_step/sec: 51.3347\n",
      "INFO:tensorflow:step = 101, loss = 0.267228 (1.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3834\n",
      "INFO:tensorflow:step = 201, loss = 0.112021 (1.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.687\n",
      "INFO:tensorflow:step = 301, loss = 0.046028 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.6039\n",
      "INFO:tensorflow:step = 401, loss = 0.0288393 (1.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.4668\n",
      "INFO:tensorflow:step = 501, loss = 0.0197099 (1.948 sec)\n",
      "INFO:tensorflow:global_step/sec: 46.2749\n",
      "INFO:tensorflow:step = 601, loss = 0.0136949 (2.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.356\n",
      "INFO:tensorflow:step = 701, loss = 0.00980746 (1.910 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.8135\n",
      "INFO:tensorflow:step = 801, loss = 0.00756388 (1.930 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4384\n",
      "INFO:tensorflow:step = 901, loss = 0.00607897 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.687\n",
      "INFO:tensorflow:step = 1001, loss = 0.00498943 (1.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.2466\n",
      "INFO:tensorflow:step = 1101, loss = 0.00414195 (1.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.0833\n",
      "INFO:tensorflow:step = 1201, loss = 0.00344082 (1.921 sec)\n",
      "INFO:tensorflow:global_step/sec: 47.9846\n",
      "INFO:tensorflow:step = 1301, loss = 0.0028398 (2.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.687\n",
      "INFO:tensorflow:step = 1401, loss = 0.00233108 (1.898 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4384\n",
      "INFO:tensorflow:step = 1501, loss = 0.00191532 (1.907 sec)\n",
      "INFO:tensorflow:global_step/sec: 45.5789\n",
      "INFO:tensorflow:step = 1601, loss = 0.0015928 (2.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 51.7866\n",
      "INFO:tensorflow:step = 1701, loss = 0.00133937 (1.926 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.3286\n",
      "INFO:tensorflow:step = 1801, loss = 0.00114154 (1.911 sec)\n",
      "INFO:tensorflow:global_step/sec: 52.4659\n",
      "INFO:tensorflow:step = 1901, loss = 0.000986095 (1.906 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into C:\\Users\\A664107\\AppData\\Local\\Temp\\tmpo335zwhp\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000862976.\n",
      "WARNING:tensorflow:From <ipython-input-9-e843a3cd33e0>:5: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-9-e843a3cd33e0>:5: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\head.py:625: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "INFO:tensorflow:Starting evaluation at 2017-07-13-09:39:50\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\A664107\\AppData\\Local\\Temp\\tmpo335zwhp\\model.ckpt-2000\n",
      "INFO:tensorflow:Finished evaluation at 2017-07-13-09:39:50\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.988, global_step = 2000, loss = 0.0427849\n",
      "Accuracy : 0.988000\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "classifier.fit(x = training_set.data, y = training_set.target, steps = 2000)\n",
    "\n",
    "#evaluate the model\n",
    "accuracy_score = classifier.evaluate(x = test_set.data, y = test_set.target)[\"accuracy\"]\n",
    "print(\"Accuracy : {0:f}\".format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   business_classification_BUSICRI  business_classification_BUSIIMP  \\\n",
      "0                              0.0                              0.0   \n",
      "\n",
      "   business_classification_MISCRI  business_classification_NONCRI  \\\n",
      "0                             0.0                             0.0   \n",
      "\n",
      "   business_classification_PRODIMP  business_impact_LARGE_FIN_COMP  \\\n",
      "0                              1.0                             1.0   \n",
      "\n",
      "   business_impact_SOME_FIN_PROD  business_impact_SOME_PROD  \\\n",
      "0                            0.0                        0.0   \n",
      "\n",
      "   business_relevance_INTSUP  business_relevance_REVGEN  \\\n",
      "0                        0.0                        0.0   \n",
      "\n",
      "           ...            dependencies.Operating.System.Dependent_0  \\\n",
      "0          ...                                                  1.0   \n",
      "\n",
      "   dependencies.Operating.System.Dependent_1  IsPassPlatAvail_0  \\\n",
      "0                                        0.0                1.0   \n",
      "\n",
      "   IsPassPlatAvail_1  IsHarwareSupported_0  IsHarwareSupported_1  \\\n",
      "0                0.0                   0.0                   1.0   \n",
      "\n",
      "   IsOSSupported_0  IsOSSupported_1  IsPlatformSupported_1  \\\n",
      "0              0.0              1.0                    1.0   \n",
      "\n",
      "   IsDatabaseSupported_1  \n",
      "0                    1.0  \n",
      "\n",
      "[1 rows x 1095 columns]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('topredict.csv', usecols = headers)\n",
    "for f in headers:\n",
    "    test[f] = test[f].astype('category')\n",
    "    test[f].cat.set_categories(categories[f],inplace=True)\n",
    "\n",
    "new_test = pd.get_dummies(test,columns = headers)\n",
    "print(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:347: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01.\n",
      "Instructions for updating:\n",
      "Please switch to predict_classes, or set `outputs` argument.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\estimators\\dnn.py:433: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:float64 is not supported by many models, consider casting to float32.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\A664107\\AppData\\Local\\Temp\\tmpo335zwhp\\model.ckpt-2000\n",
      "Predictions: [3]\n",
      "Index(['Migrate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "y = list(classifier.predict(new_test, as_iterable=True))\n",
    "print('Predictions: {}'.format(str(y)))\n",
    "print(df_labels[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting trained model to ~/Documents/cloudcomputing/Project/RNN_timeseries/\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No variables to save",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0bf420451bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minit_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"init_op\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'export'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pad_step_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_step_number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1159\u001b[0m           \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m           \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to save\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       self.saver_def = self._builder.build(\n",
      "\u001b[0;31mValueError\u001b[0m: No variables to save"
     ]
    }
   ],
   "source": [
    "import os\n",
    "export_path = \"~/Documents/cloudcomputing/Project/RNN_timeseries/\"\n",
    "print (\"Exporting trained model to %s\" % export_path)\n",
    "init_op = tf.group(tf.tables_initializer(), name=\"init_op\")\n",
    "\n",
    "saver = tf.train.Saver(sharded = True)\n",
    "saver.save(sess, os.path.join(export_path, 'export'))\n",
    "\n",
    "print(\"Done exporting!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
